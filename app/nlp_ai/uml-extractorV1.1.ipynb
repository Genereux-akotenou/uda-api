{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"./header.png\" style=\"width: 100%; height: 1em;\" alt=\"banner\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<b>UML Extractor<b/> - \n",
    "<span>v1.1</span>\n",
    "<br>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install stanza\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "#import nltk\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "fr_stopwords = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /stanfordnlp/stanza-resources/main/resources_1.3.0.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f08666c2490>: Failed to establish a new connection: [Errno -2] Name or service not known'))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mgaierror\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001B[0m in \u001B[0;36m_new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    156\u001B[0m             conn = connection.create_connection(\n\u001B[0;32m--> 157\u001B[0;31m                 \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dns_host\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mport\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mextra_kw\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    158\u001B[0m             )\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001B[0m in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mres\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msocket\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetaddrinfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhost\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mport\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfamily\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msocket\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSOCK_STREAM\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m         \u001B[0maf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msocktype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mproto\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcanonname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msa\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/socket.py\u001B[0m in \u001B[0;36mgetaddrinfo\u001B[0;34m(host, port, family, type, proto, flags)\u001B[0m\n\u001B[1;32m    751\u001B[0m     \u001B[0maddrlist\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 752\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mres\u001B[0m \u001B[0;32min\u001B[0m \u001B[0m_socket\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetaddrinfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhost\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mport\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfamily\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mproto\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflags\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    753\u001B[0m         \u001B[0maf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msocktype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mproto\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcanonname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msa\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mgaierror\u001B[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNewConnectionError\u001B[0m                        Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    671\u001B[0m                 \u001B[0mheaders\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 672\u001B[0;31m                 \u001B[0mchunked\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mchunked\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    673\u001B[0m             )\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36m_make_request\u001B[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[1;32m    375\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 376\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_conn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    377\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mSocketTimeout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mBaseSSLError\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36m_validate_conn\u001B[0;34m(self, conn)\u001B[0m\n\u001B[1;32m    993\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"sock\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# AppEngine might not have  `.sock`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 994\u001B[0;31m             \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    995\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001B[0m in \u001B[0;36mconnect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    299\u001B[0m         \u001B[0;31m# Add certificate verification\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 300\u001B[0;31m         \u001B[0mconn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_new_conn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    301\u001B[0m         \u001B[0mhostname\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhost\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001B[0m in \u001B[0;36m_new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    168\u001B[0m             raise NewConnectionError(\n\u001B[0;32m--> 169\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Failed to establish a new connection: %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    170\u001B[0m             )\n",
      "\u001B[0;31mNewConnectionError\u001B[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f08666c2490>: Failed to establish a new connection: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001B[0m in \u001B[0;36msend\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    448\u001B[0m                     \u001B[0mretries\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_retries\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 449\u001B[0;31m                     \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    450\u001B[0m                 )\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[1;32m    719\u001B[0m             retries = retries.increment(\n\u001B[0;32m--> 720\u001B[0;31m                 \u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merror\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_pool\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_stacktrace\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexc_info\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    721\u001B[0m             )\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\u001B[0m in \u001B[0;36mincrement\u001B[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[1;32m    435\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mnew_retry\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_exhausted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 436\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mMaxRetryError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_pool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merror\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mResponseError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcause\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    437\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mMaxRetryError\u001B[0m: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /stanfordnlp/stanza-resources/main/resources_1.3.0.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f08666c2490>: Failed to establish a new connection: [Errno -2] Name or service not known'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-a2072f5868d3>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#we'll dowload french model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mstanza\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdownload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'fr'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/stanza/resources/common.py\u001B[0m in \u001B[0;36mdownload\u001B[0;34m(lang, model_dir, package, processors, logging_level, verbose, resources_url, resources_branch, resources_version, model_url, proxies)\u001B[0m\n\u001B[1;32m    408\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m     download_resources_json(model_dir, resources_url, resources_branch,\n\u001B[0;32m--> 410\u001B[0;31m                             resources_version, proxies)\n\u001B[0m\u001B[1;32m    411\u001B[0m     \u001B[0;31m# unpack results\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    412\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'resources.json'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mfin\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/stanza/resources/common.py\u001B[0m in \u001B[0;36mdownload_resources_json\u001B[0;34m(model_dir, resources_url, resources_branch, resources_version, proxies)\u001B[0m\n\u001B[1;32m    362\u001B[0m         \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'resources.json'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    363\u001B[0m         \u001B[0mproxies\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 364\u001B[0;31m         \u001B[0mraise_for_status\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    365\u001B[0m     )\n\u001B[1;32m    366\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/stanza/resources/common.py\u001B[0m in \u001B[0;36mrequest_file\u001B[0;34m(url, path, proxies, md5, raise_for_status)\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf'File exists: {path}.'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    141\u001B[0m         \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 142\u001B[0;31m     \u001B[0mdownload_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mproxies\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mraise_for_status\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    143\u001B[0m     \u001B[0massert_file_exists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmd5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/stanza/resources/common.py\u001B[0m in \u001B[0;36mdownload_file\u001B[0;34m(url, path, proxies, raise_for_status)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \"\"\"\n\u001B[1;32m    116\u001B[0m     \u001B[0mverbose\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlevel\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m     \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mrequests\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstream\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mproxies\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mproxies\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'wb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m         \u001B[0mfile_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mheaders\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'content-length'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     74\u001B[0m     \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetdefault\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'allow_redirects'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 75\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'get'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     76\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0;31m# cases, and look like a memory leak in others.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0msessions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSession\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 60\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0murl\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0murl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     61\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001B[0m in \u001B[0;36mrequest\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    531\u001B[0m         }\n\u001B[1;32m    532\u001B[0m         \u001B[0msend_kwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msettings\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 533\u001B[0;31m         \u001B[0mresp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprep\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0msend_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    534\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    535\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mresp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001B[0m in \u001B[0;36msend\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    644\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    645\u001B[0m         \u001B[0;31m# Send the request\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 646\u001B[0;31m         \u001B[0mr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madapter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    647\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    648\u001B[0m         \u001B[0;31m# Total elapsed time of the request (approximately)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001B[0m in \u001B[0;36msend\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    514\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mSSLError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequest\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 516\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mConnectionError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequest\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrequest\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    517\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    518\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mClosedPoolError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mConnectionError\u001B[0m: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /stanfordnlp/stanza-resources/main/resources_1.3.0.json (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f08666c2490>: Failed to establish a new connection: [Errno -2] Name or service not known'))"
     ]
    }
   ],
   "source": [
    "#we'll dowload french model\n",
    "stanza.download('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-26 20:56:12 INFO: Loading these models for language: fr (French):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "| ner       | wikiner |\n",
      "=======================\n",
      "\n",
      "2022-03-26 20:56:12 INFO: Use device: cpu\n",
      "2022-03-26 20:56:12 INFO: Loading: tokenize\n",
      "2022-03-26 20:56:12 INFO: Loading: mwt\n",
      "2022-03-26 20:56:12 INFO: Loading: pos\n",
      "2022-03-26 20:56:14 INFO: Loading: lemma\n",
      "2022-03-26 20:56:14 INFO: Loading: depparse\n",
      "2022-03-26 20:56:17 INFO: Loading: ner\n",
      "2022-03-26 20:56:26 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#Constructing Pipeline to get all text processsing tools\n",
    "fr_nlp = stanza.Pipeline('fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"./header.png\" style=\"width: 100%; height: 1em;\" alt=\"banner\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<b style=\"margin-bottom: 1em;\">Functions<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_nlp(text):\n",
    "    return fr_nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def statistician(text):\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    stats = {'token':0, 'stats':{}, 'freq':{}}\n",
    "\n",
    "    # Création d'un corpus de tokens par artiste\n",
    "    stats['token'] = [word for word in tokenizer.tokenize(textual_text.lower()) if word not in fr_stopwords]\n",
    "    stats['freq']  = fq = nltk.FreqDist(stats['token'])\n",
    "    stats['stats'] = {'total': len(stats['token']), 'unique': len(fq.keys())}\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatizer(doc):\n",
    "    output = []\n",
    "    for i, sentence in enumerate(doc.sentences):\n",
    "        w_lemma = [word.lemma for word in sentence.words]\n",
    "        output.append(w_lemma)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_list_of_objects(size):\n",
    "    list_of_objects = list()\n",
    "    for i in range(0,size):\n",
    "        list_of_objects.append(list())\n",
    "    return list_of_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preview(doc):\n",
    "    for i, sent in enumerate(doc.sentences):\n",
    "        print(\"[Sentence {}]\".format(i+1))\n",
    "        for word in sent.words:\n",
    "            print(\"{:12s}\\t{:12s}\\t{:6s}\\t{:d}\\t{:12s}\".format(\\\n",
    "              word.text, word.lemma, word.pos, word.head, word.deprel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def text_parser(doc):\n",
    "    tab_result = []\n",
    "    for i, sent in enumerate(doc.sentences):\n",
    "        sentence_size = len([i for i in sent.words])\n",
    "        data = init_list_of_objects(sentence_size)\n",
    "        \n",
    "        for j, word in enumerate(sent.words):\n",
    "            data[j].append(word.text)\n",
    "            data[j].append(word.lemma)\n",
    "            data[j].append(word.pos)\n",
    "            data[j].append(word.deprel)\n",
    "            data[j].append(word.head)\n",
    "        tab_result.append({\"Sentence {}\".format(i+1):data})\n",
    "        \n",
    "    print(\"-\"*55)\n",
    "    preview(doc)\n",
    "    print(\"-\"*55)\n",
    "    return tab_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cardinality_finder(sentence):\n",
    "    cardinality_reper ={'un et un seul':'1,1', 'un':'1,*', 'des':'0,*', 'plusieurs':'0,*', 'au moins':'1,*'}\n",
    "\n",
    "    for search_val in cardinality_reper.keys():\n",
    "        #print(search_val)\n",
    "        combined = \"(\" + \")|(\".join(search_val) + \")\"\n",
    "        \n",
    "        if search_val in sentence:\n",
    "        #if re.match(combined, sentence):\n",
    "            #print(\"{} -- {}\".format(sentence, search_val))\n",
    "            return cardinality_reper[search_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def class_drawer(sentence):\n",
    "    #we make our sentence\n",
    "    sentence_text = ' '.join([word[0].lower() for word in sentence])\n",
    "    #let's try to identify the classes and relation\n",
    "    valuable = [word for word in sentence if (word[2] == 'NOUN' or word[2] == 'VERB' or word[2] == 'DET')]\n",
    "    #We start from the assumption that the classes are noun\n",
    "    potential_class = [word for word in sentence if word[2] == 'NOUN']\n",
    "    #Relation will be base verb of sentences\n",
    "    potential_relation = [word for word in sentence if word[2] == 'VERB']    \n",
    "    #Let get cardinality right now\n",
    "    potential_cardinality = []\n",
    "    \n",
    "    for _pos, _class in enumerate(potential_class):\n",
    "        seg = ''\n",
    "        if _pos == 0:\n",
    "            for i in range(0, sentence_text.index(_class[0])):\n",
    "                seg += sentence_text[i]\n",
    "            potential_cardinality.append(cardinality_finder(seg))\n",
    "        else:\n",
    "            for i in range(sentence_text.index(potential_class[_pos-1][0]), sentence_text.index(_class[0])):\n",
    "                seg += sentence_text[i]\n",
    "            potential_cardinality.append(cardinality_finder(seg))\n",
    "        \n",
    "            \n",
    "    potential_cardinality_data = [word for word in sentence if word[2] == 'DET']\n",
    "    \n",
    "    verb_xcomp = [verb[1] for verb in potential_relation if verb[3] == 'xcomp']\n",
    "    #print(verb_xcomp)\n",
    "    if verb_xcomp == []:\n",
    "        verb_xcomp = [verb[1] for verb in potential_relation if verb[3] == 'root']\n",
    "    #print(verb_xcomp)\n",
    "    \n",
    "    return [potential_class[0][1], [potential_cardinality[0]], verb_xcomp[0] if(len(verb_xcomp) >= 0) else [], [potential_cardinality[1]], potential_class[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Step1 : text parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def subject_finder(plain_text):\n",
    "    doc = fr_nlp(plain_text)\n",
    "    subject = []\n",
    "\n",
    "    for word in doc.sentences[0].words:\n",
    "        if 'NOUN' == word.pos:\n",
    "            subject.append(word.lemma)\n",
    "            \n",
    "    return subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def atomic_sentence_maker(plain_text, verbose=True):\n",
    "    doc =  fr_nlp(plain_text)\n",
    "    result = []\n",
    "    \n",
    "    for sentence in doc.sentences:\n",
    "        #TEXT TO SVO FORMAT\n",
    "        subject = []\n",
    "        new_sentence = \"\"\n",
    "        potential_verb = []\n",
    "        for word in sentence.words:\n",
    "            if 'VERB' in word.pos or 'AUX' in word.pos:\n",
    "                potential_verb.append(word.lemma)\n",
    "            if 'VERB' in word.pos or 'root' in word.deprel:\n",
    "                potential_verb.insert(0, word.lemma)\n",
    "            if 'VERB' in word.pos and 'xcomp' in word.deprel:\n",
    "                potential_verb.insert(0, word.lemma)\n",
    "            new_sentence += word.lemma+\" \"\n",
    "            \n",
    "        if len(potential_verb) == 0:\n",
    "            continue\n",
    "            \n",
    "        divided_sentence = re.split(potential_verb[0], str(new_sentence))\n",
    "        subject = subject_finder(divided_sentence[0])\n",
    "        \n",
    "        broken_sentence = re.split(',|;| et | ou ', str(divided_sentence[1]))\n",
    "        \n",
    "        for i, bloc in enumerate(broken_sentence):\n",
    "            other_noun = []\n",
    "            for word in fr_nlp(bloc).sentences[0].words:\n",
    "                if 'NOUN' in word.pos:\n",
    "                    other_noun.append(word.lemma)\n",
    "            broken_sentence[i] = other_noun[0] + '.'\n",
    "        \n",
    "        svo = []\n",
    "        for sub in subject:\n",
    "            index = sub + \" \" + potential_verb[0] + \" \"\n",
    "            for bloc in broken_sentence:\n",
    "                svo.append(index + bloc)\n",
    "                \n",
    "        #SENTENCE GROUPER\n",
    "        for atom in svo:\n",
    "            result.append(atom)\n",
    "                \n",
    "        if verbose:\n",
    "            #DEBUG VIEWER\n",
    "            print(svo)\n",
    "            print('-'*20)\n",
    "            #------------\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_input_text():\n",
    "    plain_text = str(input('Input : '))\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#€xtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def class_extractor(sentence_list, verbose=True):\n",
    "    class_with_desc = {}\n",
    "    potential_class = []\n",
    "    potential_attribute = []\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        sentence_tab = sentence[:-1].split(' ')\n",
    "        potential_class.append(sentence_tab[0])\n",
    "        potential_attribute.append(sentence_tab[2])\n",
    "        class_with_desc[sentence_tab[0]] = []\n",
    "        \n",
    "    for _class in potential_class:\n",
    "        if _class in potential_attribute:\n",
    "            potential_attribute.remove(_class)\n",
    "        \n",
    "    for sentence in sentence_list:\n",
    "        sentence_tab = sentence[:-1].split(' ')\n",
    "        if sentence_tab[2] in potential_attribute:\n",
    "            class_with_desc[sentence_tab[0]].append(sentence_tab[2])\n",
    "          \n",
    "    if verbose:\n",
    "        #DEBUG VIEWER\n",
    "        for _key, _class in class_with_desc.items():\n",
    "            print(\"[CLASS]\\t{} : {}\".format(_key, _class))\n",
    "        #------------\n",
    "            \n",
    "    return class_with_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def relation_extractor(sentence_list, class_desc, verbose=True):\n",
    "    potential_relation = []\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        sentence_tab = sentence[:-1].split(' ')\n",
    "        if sentence_tab[0] in class_desc.keys() and sentence_tab[2] in class_desc.keys():\n",
    "            potential_relation.append([sentence_tab[0], [sentence_tab[1], 'ASSOC'], sentence_tab[2]])\n",
    "        \n",
    "    if verbose:\n",
    "        #DEBUG VIEWER\n",
    "        for relation in potential_relation:\n",
    "            print(relation)\n",
    "        #------------\n",
    "        \n",
    "    return potential_relation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"./header.png\" style=\"width: 100%; height: 1em;\" alt=\"banner\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h6 style=\"color: red; margin-bottom: 1em;\">main.py</h6><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<b>GET INPUT</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#TEST TEXT<br>\n",
    "Un répertoire contient des fichiers. Une pièce contient des murs. Les modems et claviers sont des périphériques d’entrée-sortie. Une transaction boursière est un achat ou une vente. Un compte bancaire peut appartenir à une personne physique ou morale. Un étudiants peut suivre plusieurs cours.Un répertoire contient des fichiers. Une pièce contient des murs. Un étudiants peut suivre plusieurs cours.\n",
    "\n",
    "Les modems et claviers sont des périphériques d’entrée-sortie. Une transaction boursière est un achat ou une vente. Un répertoire contient des fichiers. Une pièce contient des murs. Un étudiant a un nom, un prénom et un matricule. Les professeurs ont un code et une matière a enseigné.\n",
    "\n",
    "Un cour est enseigné par un professeur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : Un cour est enseigné par un professeur. UN cour possède un id et un intutilé. le professeur a un email et un pot_de_passe.\n",
      "['cour enseigner professeur.']\n",
      "--------------------\n",
      "['cour posséder id.', 'cour posséder intutilé.']\n",
      "--------------------\n",
      "['professeur avoir email.', 'professeur avoir pot_de_passe.']\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "plain_text = get_input_text()\n",
    "svo = atomic_sentence_maker(plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLASS]\tcour : ['id', 'intutilé']\n",
      "[CLASS]\tprofesseur : ['email', 'pot_de_passe']\n"
     ]
    }
   ],
   "source": [
    "class_desc = class_extractor(svo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cour', ['enseigner', 'ASSOC'], 'professeur']\n"
     ]
    }
   ],
   "source": [
    "relation_desc = relation_extractor(svo, class_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<b>LEMMATIZATION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "doc = fr_nlp(\"Un cour est enseigné par un professeur.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['un', 'cour', 'être', 'enseigner', 'par', 'un', 'professeur', '.']]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<b>DOCS PARSER</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------\n",
      "[Sentence 1]\n",
      "Un          \tun          \tDET   \t2\tdet         \n",
      "cour        \tcour        \tNOUN  \t4\tnsubj:pass  \n",
      "est         \têtre        \tAUX   \t4\taux:pass    \n",
      "enseigné    \tenseigner   \tVERB  \t0\troot        \n",
      "par         \tpar         \tADP   \t7\tcase        \n",
      "un          \tun          \tDET   \t7\tdet         \n",
      "professeur  \tprofesseur  \tNOUN  \t4\tobl:agent   \n",
      ".           \t.           \tPUNCT \t4\tpunct       \n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "concept_list = text_parser(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['étudiant', ['1,*'], 'avoir', ['1,*'], 'nom']\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(concept_list):\n",
    "    print(class_drawer(sentence['Sentence {}'.format(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#GENERAL VAR\n",
    "ATTRIBUTE = []\n",
    "with open('./src/attribute_fr_dataset.txt') as attrib_file:\n",
    "    for attribute in attrib_file:\n",
    "        ATTRIBUTE.append(attribute.strip())\n",
    "        \n",
    "DESIGN_ELEMENT = []\n",
    "with open('./src/system_design_keyword.txt') as attrib_file:\n",
    "    for attribute in attrib_file:\n",
    "        DESIGN_ELEMENT.append(attribute.strip())\n",
    "\n",
    "AGREGATION_LIST = []\n",
    "with open('./src/system_design_keyword.txt') as attrib_file:\n",
    "    for attribute in attrib_file:\n",
    "        AGREGATION_LIST.append(attribute.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'nom', 'prénom', 'prix', 'matricule', 'adresse', '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATTRIBUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def class_extractor(sentence):\n",
    "    sentence_text = ' '.join([word[0].lower() for word in sentence])\n",
    "    new_doc = text_to_nlp(sentence_text)\n",
    "    tokens = []\n",
    "\n",
    "    for token in new_doc.sentences[0].tokens:\n",
    "        tokens.append(token)\n",
    "        \n",
    "    #print(tokens)\n",
    "        \n",
    "    \n",
    "    sentence = [word for word in sentence if  word[2] not in \"PUNCT\"]\n",
    "    sentence = [word for word in sentence if  word[1].lower() not in fr_stopwords]    \n",
    "    sentence = [word for word in sentence if  word[1].lower() not in DESIGN_ELEMENT]\n",
    "    sentence = [word for word in sentence if  word[1].lower() not in ATTRIBUTE]\n",
    "    sentence = [word for i, word in enumerate(sentence) if \"PER\" not in tokens[i].ner or \"MISC\" not in tokens[i].ner]\n",
    "    sentence = [word for word in sentence if  word[2] not in \"DET\" and word[2] not in \"VERB\"]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['étudiants', 'étudiant', 'NOUN', 'nsubj', 3], ['cours', 'cours', 'NOUN', 'obj', 4]]\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(concept_list):\n",
    "    print(class_extractor(sentence['Sentence {}'.format(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def relation_extractor(sentence):\n",
    "    potential_relation = [word for word in sentence if word[2] == 'VERB']    \n",
    "    \n",
    "    \n",
    "    verb_xcomp = [verb[1] for verb in potential_relation if verb[3] == 'xcomp']\n",
    "    #print(verb_xcomp)\n",
    "    if verb_xcomp == []:\n",
    "        verb_xcomp = [verb[1] for verb in potential_relation if verb[3] == 'root']\n",
    "    #print(verb_xcomp)\n",
    "    \n",
    "    nature = ['ASSOCIATION', 'AGGREGATION', 'COMPOSITION']\n",
    "    \n",
    "    \n",
    "    return [verb_xcomp[0], nature] if(len(verb_xcomp) >= 0) else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-274d8b028ada>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msentence\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconcept_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrelation_extractor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Sentence {}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-22-a73e1edad2cc>\u001B[0m in \u001B[0;36mrelation_extractor\u001B[0;34m(sentence)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mverb_xcomp\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnature\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mverb_xcomp\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'nature' is not defined"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(concept_list):\n",
    "    print(relation_extractor(sentence['Sentence {}'.format(i+1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#CLASS EXTRACTION RULES\n",
    "# ❌️ if word accur on time and it frequency lower than 2%, --> ignore. -- delete rules\n",
    "# ✅️ if word related to design elements[application, system, data], --> ignore.\n",
    "# ✅️ if word related to poeple name or location, --> ignore.\n",
    "# if word is an attribute[id, name, phone_number], --> ignore.\n",
    "# ✅️ if word is a verb or det then, --> ignore.\n",
    "# ------------------------------------------------------\n",
    "#                 user        <-- hypernym\n",
    "#                  |               \n",
    "#              --------- \n",
    "#               |     |\n",
    "#         Student    Teacher  <-- hyponym\n",
    "#            |           |\n",
    "#     ----------------  --------------------------\n",
    "#      |     |       |                   |      |   \n",
    "#  primary  college  university\n",
    "# ------------------------------------------------------\n",
    "# if word is found in hight level tree of hyponym tree, --> ignore.\n",
    "# else it probably a [CLASS].\n",
    "# if we're in Noun phrase case (Noun+Noun), if 2nd Word == attrib then 1rst is class.\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "#ATTRIBUTE EXTRACTION RULES\n",
    "# In case of noun phrase (Noun+Noun) if we've underscore mark between two noun then\n",
    "# the 1rst noun is an attribute and thee 2nd is the class.\n",
    "#\n",
    "# If word can have only one value then it's an attribute\n",
    "# based on predefined popular attributes list[ID, address, name, email, ...]\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "#RELATIONSHIP EXTRACTION RULES\n",
    "# -- OUR RELATION HAVE TO BE A VERB --\n",
    "# \n",
    "# (1) if we've the diagram [class - word - class] and word is verb then, --> AASOCIATION\n",
    "# if (1) and verb is equal to [aggregation word] then, --> AGGREGATION OR COMPISITION\n",
    "# same for DEPENDENCIES \n",
    "# \n",
    "# if CLASS1 - VERB - CLASS2 AND CLASS3 then,    --> { CLASS1 IN RELATION WHITH CLASS2\n",
    "#                                                   { CLASS1 IN RELATION WHITH CLASS3\n",
    "#\n",
    "# if CLASS1 - VERB - CLASS2 AND NOT CLASS3 then,--> { CLASS1 IN RELATION WHITH CLASS2 ONLY\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "#CARDINALITY RULES\n",
    "# Use pretrained model to identify cardinality\n",
    "# OR\n",
    "# define set of rules\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}